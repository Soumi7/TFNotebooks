{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_PG_Models.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMomQD+LLXTBuNSI1RiitVw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soumi7/TFNotebooks/blob/master/DL_PG_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc5WWkL_K_Zn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e7a6870f-3e48-4719-9fbb-31b59474c2d4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mB4z5QQO2lZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"drive/My Drive/GROUP_OF_DATASETS/CHEMISTRY.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF8uAs33RhFp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "c72ce15f-de81-4233-99f7-1836a2ac27f3"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'Unnamed: 0.1', 'date', 'ishol/week', 'group', 'name',\n",
              "       'quantity', 'unit_cogs', 'monthly_Avgtemp', 'monthly_avg_FeelsLikeC',\n",
              "       'monthly_avg_HeatIndexC', 'monthly_avg_cloudcover',\n",
              "       'monthly_avg_humidity'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVGzBihXSgXL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "1901b75e-a32d-4469-a893-7bb5923301b8"
      },
      "source": [
        "df['quantity'].describe()"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    2152.000000\n",
              "mean        4.530158\n",
              "std         9.761890\n",
              "min         0.200000\n",
              "25%         1.000000\n",
              "50%         2.000000\n",
              "75%         3.000000\n",
              "max       120.000000\n",
              "Name: quantity, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZaKGxFdSmyN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "efe88cf7-d27a-4ee0-a02b-a131d917a027"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(df['quantity'])"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1cf16f92b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B_oKYVGS7en",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f728d095-16cf-4f67-c9d8-f3b65e95388b"
      },
      "source": [
        "df['quantity'].skew()"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.102735329529392"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJhRaaijS-QW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e2455c5f-c18d-474c-baee-8cf50facbda9"
      },
      "source": [
        "df['quantity'].kurt()"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47.641620762279814"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPfjt1h_YIpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df.drop(['Unnamed: 0','Unnamed: 0.1','group'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-3WYE4Sd1_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df.drop(['date'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZgn-XUIYio5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "be36c38c-0a83-4041-86c6-298df9577ab6"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ishol/week', 'name', 'quantity', 'unit_cogs', 'monthly_Avgtemp',\n",
              "       'monthly_avg_FeelsLikeC', 'monthly_avg_HeatIndexC',\n",
              "       'monthly_avg_cloudcover', 'monthly_avg_humidity'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF0N-FXzeZ9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwgldOKvecDu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "b3c51e4a-1f0b-4094-9acf-34d5dd6de747"
      },
      "source": [
        "train"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ishol/week</th>\n",
              "      <th>name</th>\n",
              "      <th>quantity</th>\n",
              "      <th>unit_cogs</th>\n",
              "      <th>monthly_Avgtemp</th>\n",
              "      <th>monthly_avg_FeelsLikeC</th>\n",
              "      <th>monthly_avg_HeatIndexC</th>\n",
              "      <th>monthly_avg_cloudcover</th>\n",
              "      <th>monthly_avg_humidity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1379</th>\n",
              "      <td>10</td>\n",
              "      <td>FAIRY 500 ML CYTRYNA</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.69</td>\n",
              "      <td>15.835</td>\n",
              "      <td>14.73</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>69.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1481</th>\n",
              "      <td>10</td>\n",
              "      <td>SILAN 1_85 ORANGE</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.16</td>\n",
              "      <td>15.835</td>\n",
              "      <td>14.73</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>69.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2095</th>\n",
              "      <td>12</td>\n",
              "      <td>REKLAMOWKA Z UCHWYTEM</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.370</td>\n",
              "      <td>-3.61</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>88.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1851</th>\n",
              "      <td>9</td>\n",
              "      <td>PEDZLE ZESTAW 6 SZT</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.95</td>\n",
              "      <td>3.970</td>\n",
              "      <td>0.77</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>80.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>9</td>\n",
              "      <td>PLATKI BELLA OKRAGLE 120</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.09</td>\n",
              "      <td>-0.420</td>\n",
              "      <td>-4.84</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>82.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>688</th>\n",
              "      <td>11</td>\n",
              "      <td>PASTA DO ZEB COLODENT STRONG</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.10</td>\n",
              "      <td>15.965</td>\n",
              "      <td>15.42</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>73.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>9</td>\n",
              "      <td>NOTES DO DZIEWCZYNKI</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.59</td>\n",
              "      <td>0.165</td>\n",
              "      <td>-4.03</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>73.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>872</th>\n",
              "      <td>9</td>\n",
              "      <td>PERWOL 900 ML RENEW KOLOR</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.29</td>\n",
              "      <td>17.065</td>\n",
              "      <td>17.10</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>72.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1648</th>\n",
              "      <td>8</td>\n",
              "      <td>PATYCZKI HIGIENICZNE 200 SZT MEA</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.35</td>\n",
              "      <td>10.710</td>\n",
              "      <td>8.19</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>71.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1113</th>\n",
              "      <td>9</td>\n",
              "      <td>ZAPALKI</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.08</td>\n",
              "      <td>19.725</td>\n",
              "      <td>19.90</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>75.97</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1721 rows Ã— 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      ishol/week  ... monthly_avg_humidity\n",
              "1379          10  ...                69.23\n",
              "1481          10  ...                69.23\n",
              "2095          12  ...                88.58\n",
              "1851           9  ...                80.60\n",
              "95             9  ...                82.81\n",
              "...          ...  ...                  ...\n",
              "688           11  ...                73.23\n",
              "356            9  ...                73.23\n",
              "872            9  ...                72.30\n",
              "1648           8  ...                71.65\n",
              "1113           9  ...                75.97\n",
              "\n",
              "[1721 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od-_Z_bHevCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target=train.quantity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXBda3xrTHTA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "0dc5d730-0c94-4271-d38c-9abb57f1c74d"
      },
      "source": [
        "corrmatrix= df.corr()\n",
        "print(corrmatrix)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                        ishol/week  ...  monthly_avg_humidity\n",
            "ishol/week                1.000000  ...              0.453348\n",
            "quantity                 -0.001503  ...             -0.007148\n",
            "unit_cogs                 0.044564  ...              0.022351\n",
            "monthly_Avgtemp          -0.038260  ...             -0.670414\n",
            "monthly_avg_FeelsLikeC   -0.034126  ...             -0.664323\n",
            "monthly_avg_HeatIndexC    0.207298  ...             -0.462751\n",
            "monthly_avg_cloudcover    0.267031  ...              0.743852\n",
            "monthly_avg_humidity      0.453348  ...              1.000000\n",
            "\n",
            "[8 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fh7vq7uTd4e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "2a0e749f-00d9-4cc4-bdf4-0258a123322f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f,ax = plt.subplots(figsize=(12,9))\n",
        "sns.heatmap(corrmatrix, vmax=.8, square=True)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1cf12a7358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ8B1rEDi0v8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "25e42095-bfba-4b3a-bd02-06468d2c077b"
      },
      "source": [
        "print(train.shape)\n",
        "train=train.drop(['name'],axis=1)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1721, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUw5LNKEXjOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import mean_absolute_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TBgQEB_bsum",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "7f06c5ec-2e92-4bde-f563-af8b2d1a656c"
      },
      "source": [
        "CNN_model = Sequential()\n",
        "CNN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))\n",
        "CNN_model.add(Dense(256,kernel_initializer='normal',activation='relu'))\n",
        "CNN_model.add(Dense(256,kernel_initializer='normal',activation='relu'))\n",
        "CNN_model.add(Dense(256,kernel_initializer='normal',activation='relu'))\n",
        "CNN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
        "CNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "CNN_model.summary()"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_27 (Dense)             (None, 128)               1152      \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 166,017\n",
            "Trainable params: 166,017\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nng4jtwddHaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCLmJCeHfdWt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "9ac33ab7-b093-45aa-d1a0-c5b31787f978"
      },
      "source": [
        "train.columns"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ishol/week', 'quantity', 'unit_cogs', 'monthly_Avgtemp',\n",
              "       'monthly_avg_FeelsLikeC', 'monthly_avg_HeatIndexC',\n",
              "       'monthly_avg_cloudcover', 'monthly_avg_humidity'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_sVdbUYgh33",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "266e2165-a926-4122-8dc9-ed59a3e3383e"
      },
      "source": [
        "train.dtypes"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ishol/week                  int64\n",
              "quantity                  float64\n",
              "unit_cogs                 float64\n",
              "monthly_Avgtemp           float64\n",
              "monthly_avg_FeelsLikeC    float64\n",
              "monthly_avg_HeatIndexC      int64\n",
              "monthly_avg_cloudcover      int64\n",
              "monthly_avg_humidity      float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1u1NV5DiewF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "77c50097-d0eb-4594-acf3-5c014f1533b0"
      },
      "source": [
        "len(train)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1721"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDl4WnNciiK-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9b4ca912-533b-4fca-9581-8599ff6683d8"
      },
      "source": [
        "len(target)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1721"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpMMHT5BddNW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "06c9d3fd-30dd-40d7-8866-5c58a6df01fc"
      },
      "source": [
        "CNN_model.fit(train, target, epochs=500, batch_size=100, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1376 samples, validate on 345 samples\n",
            "Epoch 1/500\n",
            "1376/1376 [==============================] - 1s 554us/step - loss: 3.3560 - mean_absolute_error: 3.3560 - val_loss: 3.0528 - val_mean_absolute_error: 3.0528\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.05283, saving model to Weights-001--3.05283.hdf5\n",
            "Epoch 2/500\n",
            "1376/1376 [==============================] - 0s 83us/step - loss: 2.7415 - mean_absolute_error: 2.7415 - val_loss: 1.7799 - val_mean_absolute_error: 1.7799\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.05283 to 1.77994, saving model to Weights-002--1.77994.hdf5\n",
            "Epoch 3/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 1.1137 - mean_absolute_error: 1.1137 - val_loss: 0.5070 - val_mean_absolute_error: 0.5070\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.77994 to 0.50698, saving model to Weights-003--0.50698.hdf5\n",
            "Epoch 4/500\n",
            "1376/1376 [==============================] - 0s 85us/step - loss: 0.6189 - mean_absolute_error: 0.6189 - val_loss: 0.3847 - val_mean_absolute_error: 0.3847\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.50698 to 0.38474, saving model to Weights-004--0.38474.hdf5\n",
            "Epoch 5/500\n",
            "1376/1376 [==============================] - 0s 91us/step - loss: 0.4199 - mean_absolute_error: 0.4199 - val_loss: 0.2633 - val_mean_absolute_error: 0.2633\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.38474 to 0.26327, saving model to Weights-005--0.26327.hdf5\n",
            "Epoch 6/500\n",
            "1376/1376 [==============================] - 0s 89us/step - loss: 0.6236 - mean_absolute_error: 0.6236 - val_loss: 0.3451 - val_mean_absolute_error: 0.3451\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.26327\n",
            "Epoch 7/500\n",
            "1376/1376 [==============================] - 0s 87us/step - loss: 0.3182 - mean_absolute_error: 0.3182 - val_loss: 0.1745 - val_mean_absolute_error: 0.1745\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.26327 to 0.17450, saving model to Weights-007--0.17450.hdf5\n",
            "Epoch 8/500\n",
            "1376/1376 [==============================] - 0s 88us/step - loss: 0.3153 - mean_absolute_error: 0.3153 - val_loss: 0.1944 - val_mean_absolute_error: 0.1944\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.17450\n",
            "Epoch 9/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.2156 - mean_absolute_error: 0.2156 - val_loss: 0.4554 - val_mean_absolute_error: 0.4554\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.17450\n",
            "Epoch 10/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 0.3464 - mean_absolute_error: 0.3464 - val_loss: 0.1872 - val_mean_absolute_error: 0.1872\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.17450\n",
            "Epoch 11/500\n",
            "1376/1376 [==============================] - 0s 87us/step - loss: 0.1430 - mean_absolute_error: 0.1430 - val_loss: 0.1282 - val_mean_absolute_error: 0.1282\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.17450 to 0.12820, saving model to Weights-011--0.12820.hdf5\n",
            "Epoch 12/500\n",
            "1376/1376 [==============================] - 0s 83us/step - loss: 0.1739 - mean_absolute_error: 0.1739 - val_loss: 0.1232 - val_mean_absolute_error: 0.1232\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.12820 to 0.12318, saving model to Weights-012--0.12318.hdf5\n",
            "Epoch 13/500\n",
            "1376/1376 [==============================] - 0s 85us/step - loss: 0.1253 - mean_absolute_error: 0.1253 - val_loss: 0.2022 - val_mean_absolute_error: 0.2022\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.12318\n",
            "Epoch 14/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 0.1201 - mean_absolute_error: 0.1201 - val_loss: 0.2721 - val_mean_absolute_error: 0.2721\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.12318\n",
            "Epoch 15/500\n",
            "1376/1376 [==============================] - 0s 100us/step - loss: 0.2598 - mean_absolute_error: 0.2598 - val_loss: 0.4036 - val_mean_absolute_error: 0.4036\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.12318\n",
            "Epoch 16/500\n",
            "1376/1376 [==============================] - 0s 89us/step - loss: 0.4571 - mean_absolute_error: 0.4571 - val_loss: 0.1750 - val_mean_absolute_error: 0.1750\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.12318\n",
            "Epoch 17/500\n",
            "1376/1376 [==============================] - 0s 85us/step - loss: 0.2750 - mean_absolute_error: 0.2750 - val_loss: 0.1155 - val_mean_absolute_error: 0.1155\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.12318 to 0.11545, saving model to Weights-017--0.11545.hdf5\n",
            "Epoch 18/500\n",
            "1376/1376 [==============================] - 0s 90us/step - loss: 0.1338 - mean_absolute_error: 0.1338 - val_loss: 0.1101 - val_mean_absolute_error: 0.1101\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.11545 to 0.11013, saving model to Weights-018--0.11013.hdf5\n",
            "Epoch 19/500\n",
            "1376/1376 [==============================] - 0s 86us/step - loss: 0.1498 - mean_absolute_error: 0.1498 - val_loss: 0.2780 - val_mean_absolute_error: 0.2780\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.11013\n",
            "Epoch 20/500\n",
            "1376/1376 [==============================] - 0s 85us/step - loss: 0.2990 - mean_absolute_error: 0.2990 - val_loss: 0.1573 - val_mean_absolute_error: 0.1573\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.11013\n",
            "Epoch 21/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 0.3811 - mean_absolute_error: 0.3811 - val_loss: 0.2912 - val_mean_absolute_error: 0.2912\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.11013\n",
            "Epoch 22/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.3165 - mean_absolute_error: 0.3165 - val_loss: 0.2120 - val_mean_absolute_error: 0.2120\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.11013\n",
            "Epoch 23/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.1346 - mean_absolute_error: 0.1346 - val_loss: 0.0865 - val_mean_absolute_error: 0.0865\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.11013 to 0.08652, saving model to Weights-023--0.08652.hdf5\n",
            "Epoch 24/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1656 - val_mean_absolute_error: 0.1656\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.08652\n",
            "Epoch 25/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.1327 - mean_absolute_error: 0.1327 - val_loss: 0.2970 - val_mean_absolute_error: 0.2970\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.08652\n",
            "Epoch 26/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 0.2093 - mean_absolute_error: 0.2093 - val_loss: 0.2491 - val_mean_absolute_error: 0.2491\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.08652\n",
            "Epoch 27/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.3591 - mean_absolute_error: 0.3591 - val_loss: 0.2404 - val_mean_absolute_error: 0.2404\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.08652\n",
            "Epoch 28/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.4359 - mean_absolute_error: 0.4359 - val_loss: 0.3615 - val_mean_absolute_error: 0.3615\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.08652\n",
            "Epoch 29/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 0.4229 - mean_absolute_error: 0.4229 - val_loss: 0.3066 - val_mean_absolute_error: 0.3066\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.08652\n",
            "Epoch 30/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.2080 - mean_absolute_error: 0.2080 - val_loss: 0.1500 - val_mean_absolute_error: 0.1500\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.08652\n",
            "Epoch 31/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1296 - mean_absolute_error: 0.1296 - val_loss: 0.2397 - val_mean_absolute_error: 0.2397\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.08652\n",
            "Epoch 32/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.1529 - mean_absolute_error: 0.1529 - val_loss: 0.0634 - val_mean_absolute_error: 0.0634\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.08652 to 0.06340, saving model to Weights-032--0.06340.hdf5\n",
            "Epoch 33/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 0.1629 - mean_absolute_error: 0.1629 - val_loss: 0.3213 - val_mean_absolute_error: 0.3213\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.06340\n",
            "Epoch 34/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.1201 - mean_absolute_error: 0.1201 - val_loss: 0.1299 - val_mean_absolute_error: 0.1299\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.06340\n",
            "Epoch 35/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1902 - mean_absolute_error: 0.1902 - val_loss: 0.3661 - val_mean_absolute_error: 0.3661\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.06340\n",
            "Epoch 36/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.3127 - mean_absolute_error: 0.3127 - val_loss: 0.2411 - val_mean_absolute_error: 0.2411\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.06340\n",
            "Epoch 37/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.2281 - mean_absolute_error: 0.2281 - val_loss: 0.0984 - val_mean_absolute_error: 0.0984\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.06340\n",
            "Epoch 38/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1147 - mean_absolute_error: 0.1147 - val_loss: 0.0632 - val_mean_absolute_error: 0.0632\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.06340 to 0.06321, saving model to Weights-038--0.06321.hdf5\n",
            "Epoch 39/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.1059 - mean_absolute_error: 0.1059 - val_loss: 0.1027 - val_mean_absolute_error: 0.1027\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.06321\n",
            "Epoch 40/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.1476 - mean_absolute_error: 0.1476 - val_loss: 0.0992 - val_mean_absolute_error: 0.0992\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.06321\n",
            "Epoch 41/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.1070 - mean_absolute_error: 0.1070 - val_loss: 0.1122 - val_mean_absolute_error: 0.1122\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.06321\n",
            "Epoch 42/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.1272 - mean_absolute_error: 0.1272 - val_loss: 0.1093 - val_mean_absolute_error: 0.1093\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.06321\n",
            "Epoch 43/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1655 - mean_absolute_error: 0.1655 - val_loss: 0.1400 - val_mean_absolute_error: 0.1400\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.06321\n",
            "Epoch 44/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.1606 - mean_absolute_error: 0.1606 - val_loss: 0.0733 - val_mean_absolute_error: 0.0733\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.06321\n",
            "Epoch 45/500\n",
            "1376/1376 [==============================] - 0s 86us/step - loss: 0.1550 - mean_absolute_error: 0.1550 - val_loss: 0.1604 - val_mean_absolute_error: 0.1604\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.06321\n",
            "Epoch 46/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.1733 - mean_absolute_error: 0.1733 - val_loss: 0.1285 - val_mean_absolute_error: 0.1285\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.06321\n",
            "Epoch 47/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.1638 - mean_absolute_error: 0.1638 - val_loss: 0.1009 - val_mean_absolute_error: 0.1009\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.06321\n",
            "Epoch 48/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.1678 - mean_absolute_error: 0.1678 - val_loss: 0.1961 - val_mean_absolute_error: 0.1961\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.06321\n",
            "Epoch 49/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.1375 - mean_absolute_error: 0.1375 - val_loss: 0.1136 - val_mean_absolute_error: 0.1136\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.06321\n",
            "Epoch 50/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.1334 - mean_absolute_error: 0.1334 - val_loss: 0.1440 - val_mean_absolute_error: 0.1440\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.06321\n",
            "Epoch 51/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.1286 - mean_absolute_error: 0.1286 - val_loss: 0.0856 - val_mean_absolute_error: 0.0856\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.06321\n",
            "Epoch 52/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0827 - mean_absolute_error: 0.0827 - val_loss: 0.0768 - val_mean_absolute_error: 0.0768\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.06321\n",
            "Epoch 53/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0832 - mean_absolute_error: 0.0832 - val_loss: 0.1352 - val_mean_absolute_error: 0.1352\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.06321\n",
            "Epoch 54/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.1373 - mean_absolute_error: 0.1373 - val_loss: 0.1754 - val_mean_absolute_error: 0.1754\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.06321\n",
            "Epoch 55/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.1504 - mean_absolute_error: 0.1504 - val_loss: 0.1018 - val_mean_absolute_error: 0.1018\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.06321\n",
            "Epoch 56/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.1190 - mean_absolute_error: 0.1190 - val_loss: 0.1113 - val_mean_absolute_error: 0.1113\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.06321\n",
            "Epoch 57/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.1199 - mean_absolute_error: 0.1199 - val_loss: 0.1451 - val_mean_absolute_error: 0.1451\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.06321\n",
            "Epoch 58/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.1180 - mean_absolute_error: 0.1180 - val_loss: 0.0412 - val_mean_absolute_error: 0.0412\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.06321 to 0.04122, saving model to Weights-058--0.04122.hdf5\n",
            "Epoch 59/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.1443 - mean_absolute_error: 0.1443 - val_loss: 0.2118 - val_mean_absolute_error: 0.2118\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.04122\n",
            "Epoch 60/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.0531 - val_mean_absolute_error: 0.0531\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.04122\n",
            "Epoch 61/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.1507 - mean_absolute_error: 0.1507 - val_loss: 0.2251 - val_mean_absolute_error: 0.2251\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.04122\n",
            "Epoch 62/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.1202 - mean_absolute_error: 0.1202 - val_loss: 0.1039 - val_mean_absolute_error: 0.1039\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.04122\n",
            "Epoch 63/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.1785 - mean_absolute_error: 0.1785 - val_loss: 0.1488 - val_mean_absolute_error: 0.1488\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.04122\n",
            "Epoch 64/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0863 - mean_absolute_error: 0.0863 - val_loss: 0.0736 - val_mean_absolute_error: 0.0736\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.04122\n",
            "Epoch 65/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.1277 - mean_absolute_error: 0.1277 - val_loss: 0.0697 - val_mean_absolute_error: 0.0697\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.04122\n",
            "Epoch 66/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 0.0872 - mean_absolute_error: 0.0872 - val_loss: 0.1554 - val_mean_absolute_error: 0.1554\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.04122\n",
            "Epoch 67/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0780 - mean_absolute_error: 0.0780 - val_loss: 0.0580 - val_mean_absolute_error: 0.0580\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.04122\n",
            "Epoch 68/500\n",
            "1376/1376 [==============================] - 0s 84us/step - loss: 0.0584 - mean_absolute_error: 0.0584 - val_loss: 0.0914 - val_mean_absolute_error: 0.0914\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.04122\n",
            "Epoch 69/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0837 - mean_absolute_error: 0.0837 - val_loss: 0.0379 - val_mean_absolute_error: 0.0379\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.04122 to 0.03785, saving model to Weights-069--0.03785.hdf5\n",
            "Epoch 70/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0643 - mean_absolute_error: 0.0643 - val_loss: 0.0792 - val_mean_absolute_error: 0.0792\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.03785\n",
            "Epoch 71/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.1090 - mean_absolute_error: 0.1090 - val_loss: 0.1352 - val_mean_absolute_error: 0.1352\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.03785\n",
            "Epoch 72/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 0.1172 - mean_absolute_error: 0.1172 - val_loss: 0.0741 - val_mean_absolute_error: 0.0741\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.03785\n",
            "Epoch 73/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 0.0899 - mean_absolute_error: 0.0899 - val_loss: 0.0504 - val_mean_absolute_error: 0.0504\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.03785\n",
            "Epoch 74/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1153 - mean_absolute_error: 0.1153 - val_loss: 0.1157 - val_mean_absolute_error: 0.1157\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.03785\n",
            "Epoch 75/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.1697 - mean_absolute_error: 0.1697 - val_loss: 0.1549 - val_mean_absolute_error: 0.1549\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.03785\n",
            "Epoch 76/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1068 - mean_absolute_error: 0.1068 - val_loss: 0.1079 - val_mean_absolute_error: 0.1079\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.03785\n",
            "Epoch 77/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.1299 - mean_absolute_error: 0.1299 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.03785\n",
            "Epoch 78/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.1125 - mean_absolute_error: 0.1125 - val_loss: 0.0487 - val_mean_absolute_error: 0.0487\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.03785\n",
            "Epoch 79/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.1007 - mean_absolute_error: 0.1007 - val_loss: 0.0969 - val_mean_absolute_error: 0.0969\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.03785\n",
            "Epoch 80/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0835 - mean_absolute_error: 0.0835 - val_loss: 0.1409 - val_mean_absolute_error: 0.1409\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.03785\n",
            "Epoch 81/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.1148 - mean_absolute_error: 0.1148 - val_loss: 0.1442 - val_mean_absolute_error: 0.1442\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.03785\n",
            "Epoch 82/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.1500 - mean_absolute_error: 0.1500 - val_loss: 0.1875 - val_mean_absolute_error: 0.1875\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.03785\n",
            "Epoch 83/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1460 - mean_absolute_error: 0.1460 - val_loss: 0.1226 - val_mean_absolute_error: 0.1226\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.03785\n",
            "Epoch 84/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.1118 - mean_absolute_error: 0.1118 - val_loss: 0.0310 - val_mean_absolute_error: 0.0310\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.03785 to 0.03100, saving model to Weights-084--0.03100.hdf5\n",
            "Epoch 85/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 0.1246 - mean_absolute_error: 0.1246 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.03100\n",
            "Epoch 86/500\n",
            "1376/1376 [==============================] - 0s 67us/step - loss: 0.0996 - mean_absolute_error: 0.0996 - val_loss: 0.0361 - val_mean_absolute_error: 0.0361\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.03100\n",
            "Epoch 87/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.1027 - mean_absolute_error: 0.1027 - val_loss: 0.2878 - val_mean_absolute_error: 0.2878\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.03100\n",
            "Epoch 88/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.2185 - mean_absolute_error: 0.2185 - val_loss: 0.1902 - val_mean_absolute_error: 0.1902\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.03100\n",
            "Epoch 89/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.1746 - mean_absolute_error: 0.1746 - val_loss: 0.1806 - val_mean_absolute_error: 0.1806\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.03100\n",
            "Epoch 90/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1349 - mean_absolute_error: 0.1349 - val_loss: 0.0896 - val_mean_absolute_error: 0.0896\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.03100\n",
            "Epoch 91/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1331 - val_mean_absolute_error: 0.1331\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.03100\n",
            "Epoch 92/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.0865 - val_mean_absolute_error: 0.0865\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.03100\n",
            "Epoch 93/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 0.1007 - mean_absolute_error: 0.1007 - val_loss: 0.0319 - val_mean_absolute_error: 0.0319\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.03100\n",
            "Epoch 94/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0883 - mean_absolute_error: 0.0883 - val_loss: 0.2414 - val_mean_absolute_error: 0.2414\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.03100\n",
            "Epoch 95/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.2351 - mean_absolute_error: 0.2351 - val_loss: 0.3758 - val_mean_absolute_error: 0.3758\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.03100\n",
            "Epoch 96/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.2109 - mean_absolute_error: 0.2109 - val_loss: 0.1111 - val_mean_absolute_error: 0.1111\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.03100\n",
            "Epoch 97/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 0.1059 - mean_absolute_error: 0.1059 - val_loss: 0.0645 - val_mean_absolute_error: 0.0645\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.03100\n",
            "Epoch 98/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0755 - mean_absolute_error: 0.0755 - val_loss: 0.0777 - val_mean_absolute_error: 0.0777\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.03100\n",
            "Epoch 99/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0511 - mean_absolute_error: 0.0511 - val_loss: 0.0555 - val_mean_absolute_error: 0.0555\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.03100\n",
            "Epoch 100/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.1145 - mean_absolute_error: 0.1145 - val_loss: 0.1441 - val_mean_absolute_error: 0.1441\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.03100\n",
            "Epoch 101/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1343 - mean_absolute_error: 0.1343 - val_loss: 0.0935 - val_mean_absolute_error: 0.0935\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.03100\n",
            "Epoch 102/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0690 - mean_absolute_error: 0.0690 - val_loss: 0.0378 - val_mean_absolute_error: 0.0378\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.03100\n",
            "Epoch 103/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0708 - mean_absolute_error: 0.0708 - val_loss: 0.0741 - val_mean_absolute_error: 0.0741\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.03100\n",
            "Epoch 104/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 0.0925 - mean_absolute_error: 0.0925 - val_loss: 0.1611 - val_mean_absolute_error: 0.1611\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.03100\n",
            "Epoch 105/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1120 - mean_absolute_error: 0.1120 - val_loss: 0.0837 - val_mean_absolute_error: 0.0837\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.03100\n",
            "Epoch 106/500\n",
            "1376/1376 [==============================] - 0s 83us/step - loss: 0.0998 - mean_absolute_error: 0.0998 - val_loss: 0.1129 - val_mean_absolute_error: 0.1129\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.03100\n",
            "Epoch 107/500\n",
            "1376/1376 [==============================] - 0s 87us/step - loss: 0.0984 - mean_absolute_error: 0.0984 - val_loss: 0.1034 - val_mean_absolute_error: 0.1034\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.03100\n",
            "Epoch 108/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0631 - mean_absolute_error: 0.0631 - val_loss: 0.0458 - val_mean_absolute_error: 0.0458\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.03100\n",
            "Epoch 109/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0840 - mean_absolute_error: 0.0840 - val_loss: 0.0817 - val_mean_absolute_error: 0.0817\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.03100\n",
            "Epoch 110/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1031 - mean_absolute_error: 0.1031 - val_loss: 0.1538 - val_mean_absolute_error: 0.1538\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.03100\n",
            "Epoch 111/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.1275 - mean_absolute_error: 0.1275 - val_loss: 0.0865 - val_mean_absolute_error: 0.0865\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.03100\n",
            "Epoch 112/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 0.0781 - mean_absolute_error: 0.0781 - val_loss: 0.1555 - val_mean_absolute_error: 0.1555\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.03100\n",
            "Epoch 113/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 0.0897 - mean_absolute_error: 0.0897 - val_loss: 0.1678 - val_mean_absolute_error: 0.1678\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.03100\n",
            "Epoch 114/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0833 - mean_absolute_error: 0.0833 - val_loss: 0.1066 - val_mean_absolute_error: 0.1066\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.03100\n",
            "Epoch 115/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0746 - mean_absolute_error: 0.0746 - val_loss: 0.0492 - val_mean_absolute_error: 0.0492\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.03100\n",
            "Epoch 116/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.1022 - mean_absolute_error: 0.1022 - val_loss: 0.1249 - val_mean_absolute_error: 0.1249\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.03100\n",
            "Epoch 117/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.1435 - mean_absolute_error: 0.1435 - val_loss: 0.1417 - val_mean_absolute_error: 0.1417\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.03100\n",
            "Epoch 118/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1377 - mean_absolute_error: 0.1377 - val_loss: 0.1132 - val_mean_absolute_error: 0.1132\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.03100\n",
            "Epoch 119/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.1265 - mean_absolute_error: 0.1265 - val_loss: 0.1445 - val_mean_absolute_error: 0.1445\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.03100\n",
            "Epoch 120/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 0.1597 - mean_absolute_error: 0.1597 - val_loss: 0.0992 - val_mean_absolute_error: 0.0992\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.03100\n",
            "Epoch 121/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0909 - mean_absolute_error: 0.0909 - val_loss: 0.0409 - val_mean_absolute_error: 0.0409\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.03100\n",
            "Epoch 122/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0777 - mean_absolute_error: 0.0777 - val_loss: 0.0574 - val_mean_absolute_error: 0.0574\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.03100\n",
            "Epoch 123/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0879 - mean_absolute_error: 0.0879 - val_loss: 0.0525 - val_mean_absolute_error: 0.0525\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.03100\n",
            "Epoch 124/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0684 - mean_absolute_error: 0.0684 - val_loss: 0.0632 - val_mean_absolute_error: 0.0632\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.03100\n",
            "Epoch 125/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0523 - mean_absolute_error: 0.0523 - val_loss: 0.0614 - val_mean_absolute_error: 0.0614\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.03100\n",
            "Epoch 126/500\n",
            "1376/1376 [==============================] - 0s 84us/step - loss: 0.0571 - mean_absolute_error: 0.0571 - val_loss: 0.0886 - val_mean_absolute_error: 0.0886\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.03100\n",
            "Epoch 127/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0930 - mean_absolute_error: 0.0930 - val_loss: 0.0710 - val_mean_absolute_error: 0.0710\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.03100\n",
            "Epoch 128/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0835 - mean_absolute_error: 0.0835 - val_loss: 0.0256 - val_mean_absolute_error: 0.0256\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.03100 to 0.02562, saving model to Weights-128--0.02562.hdf5\n",
            "Epoch 129/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 0.1015 - mean_absolute_error: 0.1015 - val_loss: 0.1030 - val_mean_absolute_error: 0.1030\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.02562\n",
            "Epoch 130/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0661 - mean_absolute_error: 0.0661 - val_loss: 0.1127 - val_mean_absolute_error: 0.1127\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.02562\n",
            "Epoch 131/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0824 - mean_absolute_error: 0.0824 - val_loss: 0.0570 - val_mean_absolute_error: 0.0570\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.02562\n",
            "Epoch 132/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0540 - mean_absolute_error: 0.0540 - val_loss: 0.0514 - val_mean_absolute_error: 0.0514\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.02562\n",
            "Epoch 133/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0889 - mean_absolute_error: 0.0889 - val_loss: 0.1406 - val_mean_absolute_error: 0.1406\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.02562\n",
            "Epoch 134/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0960 - mean_absolute_error: 0.0960 - val_loss: 0.0527 - val_mean_absolute_error: 0.0527\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.02562\n",
            "Epoch 135/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0946 - mean_absolute_error: 0.0946 - val_loss: 0.1463 - val_mean_absolute_error: 0.1463\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.02562\n",
            "Epoch 136/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 0.0770 - mean_absolute_error: 0.0770 - val_loss: 0.1046 - val_mean_absolute_error: 0.1046\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.02562\n",
            "Epoch 137/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0702 - mean_absolute_error: 0.0702 - val_loss: 0.0553 - val_mean_absolute_error: 0.0553\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.02562\n",
            "Epoch 138/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0819 - mean_absolute_error: 0.0819 - val_loss: 0.0362 - val_mean_absolute_error: 0.0362\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.02562\n",
            "Epoch 139/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0624 - mean_absolute_error: 0.0624 - val_loss: 0.0526 - val_mean_absolute_error: 0.0526\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.02562\n",
            "Epoch 140/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0564 - mean_absolute_error: 0.0564 - val_loss: 0.0588 - val_mean_absolute_error: 0.0588\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.02562\n",
            "Epoch 141/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0921 - mean_absolute_error: 0.0921 - val_loss: 0.1028 - val_mean_absolute_error: 0.1028\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.02562\n",
            "Epoch 142/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.1245 - mean_absolute_error: 0.1245 - val_loss: 0.1390 - val_mean_absolute_error: 0.1390\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.02562\n",
            "Epoch 143/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0984 - mean_absolute_error: 0.0984 - val_loss: 0.0710 - val_mean_absolute_error: 0.0710\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.02562\n",
            "Epoch 144/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0862 - mean_absolute_error: 0.0862 - val_loss: 0.1750 - val_mean_absolute_error: 0.1750\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.02562\n",
            "Epoch 145/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.1337 - mean_absolute_error: 0.1337 - val_loss: 0.1386 - val_mean_absolute_error: 0.1386\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.02562\n",
            "Epoch 146/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 0.0782 - mean_absolute_error: 0.0782 - val_loss: 0.0288 - val_mean_absolute_error: 0.0288\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.02562\n",
            "Epoch 147/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0904 - mean_absolute_error: 0.0904 - val_loss: 0.2249 - val_mean_absolute_error: 0.2249\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.02562\n",
            "Epoch 148/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.1990 - mean_absolute_error: 0.1990 - val_loss: 0.1543 - val_mean_absolute_error: 0.1543\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.02562\n",
            "Epoch 149/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.0939 - val_mean_absolute_error: 0.0939\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.02562\n",
            "Epoch 150/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0856 - mean_absolute_error: 0.0856 - val_loss: 0.0593 - val_mean_absolute_error: 0.0593\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.02562\n",
            "Epoch 151/500\n",
            "1376/1376 [==============================] - 0s 67us/step - loss: 0.0735 - mean_absolute_error: 0.0735 - val_loss: 0.0398 - val_mean_absolute_error: 0.0398\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.02562\n",
            "Epoch 152/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0809 - mean_absolute_error: 0.0809 - val_loss: 0.1522 - val_mean_absolute_error: 0.1522\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.02562\n",
            "Epoch 153/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1197 - mean_absolute_error: 0.1197 - val_loss: 0.0393 - val_mean_absolute_error: 0.0393\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.02562\n",
            "Epoch 154/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0945 - mean_absolute_error: 0.0945 - val_loss: 0.0709 - val_mean_absolute_error: 0.0709\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.02562\n",
            "Epoch 155/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0945 - mean_absolute_error: 0.0945 - val_loss: 0.0795 - val_mean_absolute_error: 0.0795\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.02562\n",
            "Epoch 156/500\n",
            "1376/1376 [==============================] - 0s 84us/step - loss: 0.1183 - mean_absolute_error: 0.1183 - val_loss: 0.0913 - val_mean_absolute_error: 0.0913\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.02562\n",
            "Epoch 157/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0487 - mean_absolute_error: 0.0487 - val_loss: 0.1296 - val_mean_absolute_error: 0.1296\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.02562\n",
            "Epoch 158/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.1409 - mean_absolute_error: 0.1409 - val_loss: 0.2442 - val_mean_absolute_error: 0.2442\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.02562\n",
            "Epoch 159/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.1631 - mean_absolute_error: 0.1631 - val_loss: 0.0902 - val_mean_absolute_error: 0.0902\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.02562\n",
            "Epoch 160/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0763 - mean_absolute_error: 0.0763 - val_loss: 0.1548 - val_mean_absolute_error: 0.1548\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.02562\n",
            "Epoch 161/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.1538 - mean_absolute_error: 0.1538 - val_loss: 0.0698 - val_mean_absolute_error: 0.0698\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.02562\n",
            "Epoch 162/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0628 - mean_absolute_error: 0.0628 - val_loss: 0.0860 - val_mean_absolute_error: 0.0860\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.02562\n",
            "Epoch 163/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0716 - mean_absolute_error: 0.0716 - val_loss: 0.0446 - val_mean_absolute_error: 0.0446\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.02562\n",
            "Epoch 164/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0733 - mean_absolute_error: 0.0733 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.02562\n",
            "Epoch 165/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.1069 - mean_absolute_error: 0.1069 - val_loss: 0.2372 - val_mean_absolute_error: 0.2372\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.02562\n",
            "Epoch 166/500\n",
            "1376/1376 [==============================] - 0s 84us/step - loss: 0.1594 - mean_absolute_error: 0.1594 - val_loss: 0.1389 - val_mean_absolute_error: 0.1389\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.02562\n",
            "Epoch 167/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1272 - mean_absolute_error: 0.1272 - val_loss: 0.0975 - val_mean_absolute_error: 0.0975\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.02562\n",
            "Epoch 168/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0871 - mean_absolute_error: 0.0871 - val_loss: 0.0566 - val_mean_absolute_error: 0.0566\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.02562\n",
            "Epoch 169/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0718 - mean_absolute_error: 0.0718 - val_loss: 0.0425 - val_mean_absolute_error: 0.0425\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.02562\n",
            "Epoch 170/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0567 - mean_absolute_error: 0.0567 - val_loss: 0.0647 - val_mean_absolute_error: 0.0647\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.02562\n",
            "Epoch 171/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 0.0741 - mean_absolute_error: 0.0741 - val_loss: 0.0871 - val_mean_absolute_error: 0.0871\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.02562\n",
            "Epoch 172/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 0.0593 - mean_absolute_error: 0.0593 - val_loss: 0.0540 - val_mean_absolute_error: 0.0540\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.02562\n",
            "Epoch 173/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0761 - mean_absolute_error: 0.0761 - val_loss: 0.1085 - val_mean_absolute_error: 0.1085\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.02562\n",
            "Epoch 174/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0955 - mean_absolute_error: 0.0955 - val_loss: 0.1133 - val_mean_absolute_error: 0.1133\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.02562\n",
            "Epoch 175/500\n",
            "1376/1376 [==============================] - 0s 82us/step - loss: 0.1270 - mean_absolute_error: 0.1270 - val_loss: 0.1104 - val_mean_absolute_error: 0.1104\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.02562\n",
            "Epoch 176/500\n",
            "1376/1376 [==============================] - 0s 95us/step - loss: 0.1234 - mean_absolute_error: 0.1234 - val_loss: 0.1301 - val_mean_absolute_error: 0.1301\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.02562\n",
            "Epoch 177/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0877 - mean_absolute_error: 0.0877 - val_loss: 0.0510 - val_mean_absolute_error: 0.0510\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.02562\n",
            "Epoch 178/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0599 - mean_absolute_error: 0.0599 - val_loss: 0.0737 - val_mean_absolute_error: 0.0737\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.02562\n",
            "Epoch 179/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0702 - mean_absolute_error: 0.0702 - val_loss: 0.0709 - val_mean_absolute_error: 0.0709\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.02562\n",
            "Epoch 180/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0805 - mean_absolute_error: 0.0805 - val_loss: 0.0714 - val_mean_absolute_error: 0.0714\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.02562\n",
            "Epoch 181/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 0.1082 - mean_absolute_error: 0.1082 - val_loss: 0.2742 - val_mean_absolute_error: 0.2742\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.02562\n",
            "Epoch 182/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 0.1785 - mean_absolute_error: 0.1785 - val_loss: 0.2187 - val_mean_absolute_error: 0.2187\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.02562\n",
            "Epoch 183/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.1949 - mean_absolute_error: 0.1949 - val_loss: 0.1175 - val_mean_absolute_error: 0.1175\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.02562\n",
            "Epoch 184/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0979 - mean_absolute_error: 0.0979 - val_loss: 0.0506 - val_mean_absolute_error: 0.0506\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.02562\n",
            "Epoch 185/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 0.0602 - mean_absolute_error: 0.0602 - val_loss: 0.0525 - val_mean_absolute_error: 0.0525\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.02562\n",
            "Epoch 186/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0693 - mean_absolute_error: 0.0693 - val_loss: 0.0897 - val_mean_absolute_error: 0.0897\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.02562\n",
            "Epoch 187/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.1093 - mean_absolute_error: 0.1093 - val_loss: 0.0803 - val_mean_absolute_error: 0.0803\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.02562\n",
            "Epoch 188/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.1177 - mean_absolute_error: 0.1177 - val_loss: 0.0961 - val_mean_absolute_error: 0.0961\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.02562\n",
            "Epoch 189/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0827 - mean_absolute_error: 0.0827 - val_loss: 0.1008 - val_mean_absolute_error: 0.1008\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.02562\n",
            "Epoch 190/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0648 - mean_absolute_error: 0.0648 - val_loss: 0.0813 - val_mean_absolute_error: 0.0813\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.02562\n",
            "Epoch 191/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.1010 - mean_absolute_error: 0.1010 - val_loss: 0.1457 - val_mean_absolute_error: 0.1457\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.02562\n",
            "Epoch 192/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.1476 - mean_absolute_error: 0.1476 - val_loss: 0.1311 - val_mean_absolute_error: 0.1311\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.02562\n",
            "Epoch 193/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.1474 - mean_absolute_error: 0.1474 - val_loss: 0.1011 - val_mean_absolute_error: 0.1011\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.02562\n",
            "Epoch 194/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0774 - mean_absolute_error: 0.0774 - val_loss: 0.0409 - val_mean_absolute_error: 0.0409\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.02562\n",
            "Epoch 195/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0678 - mean_absolute_error: 0.0678 - val_loss: 0.1378 - val_mean_absolute_error: 0.1378\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.02562\n",
            "Epoch 196/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0968 - mean_absolute_error: 0.0968 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.02562\n",
            "Epoch 197/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0802 - mean_absolute_error: 0.0802 - val_loss: 0.0396 - val_mean_absolute_error: 0.0396\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.02562\n",
            "Epoch 198/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0524 - mean_absolute_error: 0.0524 - val_loss: 0.0401 - val_mean_absolute_error: 0.0401\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.02562\n",
            "Epoch 199/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0591 - mean_absolute_error: 0.0591 - val_loss: 0.1185 - val_mean_absolute_error: 0.1185\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.02562\n",
            "Epoch 200/500\n",
            "1376/1376 [==============================] - 0s 82us/step - loss: 0.0984 - mean_absolute_error: 0.0984 - val_loss: 0.1058 - val_mean_absolute_error: 0.1058\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.02562\n",
            "Epoch 201/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0959 - mean_absolute_error: 0.0959 - val_loss: 0.1744 - val_mean_absolute_error: 0.1744\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.02562\n",
            "Epoch 202/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.1548 - mean_absolute_error: 0.1548 - val_loss: 0.1127 - val_mean_absolute_error: 0.1127\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.02562\n",
            "Epoch 203/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0914 - mean_absolute_error: 0.0914 - val_loss: 0.0836 - val_mean_absolute_error: 0.0836\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.02562\n",
            "Epoch 204/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0664 - mean_absolute_error: 0.0664 - val_loss: 0.0517 - val_mean_absolute_error: 0.0517\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.02562\n",
            "Epoch 205/500\n",
            "1376/1376 [==============================] - 0s 90us/step - loss: 0.1088 - mean_absolute_error: 0.1088 - val_loss: 0.2358 - val_mean_absolute_error: 0.2358\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.02562\n",
            "Epoch 206/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.1593 - mean_absolute_error: 0.1593 - val_loss: 0.0909 - val_mean_absolute_error: 0.0909\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.02562\n",
            "Epoch 207/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1199 - mean_absolute_error: 0.1199 - val_loss: 0.0519 - val_mean_absolute_error: 0.0519\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.02562\n",
            "Epoch 208/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0939 - mean_absolute_error: 0.0939 - val_loss: 0.0765 - val_mean_absolute_error: 0.0765\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.02562\n",
            "Epoch 209/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0597 - mean_absolute_error: 0.0597 - val_loss: 0.0913 - val_mean_absolute_error: 0.0913\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.02562\n",
            "Epoch 210/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0806 - mean_absolute_error: 0.0806 - val_loss: 0.1577 - val_mean_absolute_error: 0.1577\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.02562\n",
            "Epoch 211/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.1809 - mean_absolute_error: 0.1809 - val_loss: 0.1508 - val_mean_absolute_error: 0.1508\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.02562\n",
            "Epoch 212/500\n",
            "1376/1376 [==============================] - 0s 82us/step - loss: 0.1013 - mean_absolute_error: 0.1013 - val_loss: 0.1162 - val_mean_absolute_error: 0.1162\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.02562\n",
            "Epoch 213/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0820 - mean_absolute_error: 0.0820 - val_loss: 0.0505 - val_mean_absolute_error: 0.0505\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.02562\n",
            "Epoch 214/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0476 - mean_absolute_error: 0.0476 - val_loss: 0.0826 - val_mean_absolute_error: 0.0826\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.02562\n",
            "Epoch 215/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0781 - mean_absolute_error: 0.0781 - val_loss: 0.1078 - val_mean_absolute_error: 0.1078\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.02562\n",
            "Epoch 216/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.1080 - mean_absolute_error: 0.1080 - val_loss: 0.0997 - val_mean_absolute_error: 0.0997\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.02562\n",
            "Epoch 217/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0731 - mean_absolute_error: 0.0731 - val_loss: 0.0628 - val_mean_absolute_error: 0.0628\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.02562\n",
            "Epoch 218/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0644 - mean_absolute_error: 0.0644 - val_loss: 0.0807 - val_mean_absolute_error: 0.0807\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.02562\n",
            "Epoch 219/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0547 - mean_absolute_error: 0.0547 - val_loss: 0.0453 - val_mean_absolute_error: 0.0453\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.02562\n",
            "Epoch 220/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0769 - mean_absolute_error: 0.0769 - val_loss: 0.1163 - val_mean_absolute_error: 0.1163\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.02562\n",
            "Epoch 221/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0750 - mean_absolute_error: 0.0750 - val_loss: 0.0931 - val_mean_absolute_error: 0.0931\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.02562\n",
            "Epoch 222/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0775 - mean_absolute_error: 0.0775 - val_loss: 0.1446 - val_mean_absolute_error: 0.1446\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.02562\n",
            "Epoch 223/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.1477 - mean_absolute_error: 0.1477 - val_loss: 0.1045 - val_mean_absolute_error: 0.1045\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.02562\n",
            "Epoch 224/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0734 - mean_absolute_error: 0.0734 - val_loss: 0.0388 - val_mean_absolute_error: 0.0388\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.02562\n",
            "Epoch 225/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0561 - mean_absolute_error: 0.0561 - val_loss: 0.0449 - val_mean_absolute_error: 0.0449\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.02562\n",
            "Epoch 226/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0578 - mean_absolute_error: 0.0578 - val_loss: 0.0743 - val_mean_absolute_error: 0.0743\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.02562\n",
            "Epoch 227/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0732 - mean_absolute_error: 0.0732 - val_loss: 0.1451 - val_mean_absolute_error: 0.1451\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.02562\n",
            "Epoch 228/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1940 - mean_absolute_error: 0.1940 - val_loss: 0.0767 - val_mean_absolute_error: 0.0767\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.02562\n",
            "Epoch 229/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.0549 - val_mean_absolute_error: 0.0549\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.02562\n",
            "Epoch 230/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0424 - mean_absolute_error: 0.0424 - val_loss: 0.0465 - val_mean_absolute_error: 0.0465\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.02562\n",
            "Epoch 231/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0458 - mean_absolute_error: 0.0458 - val_loss: 0.0230 - val_mean_absolute_error: 0.0230\n",
            "\n",
            "Epoch 00231: val_loss improved from 0.02562 to 0.02301, saving model to Weights-231--0.02301.hdf5\n",
            "Epoch 232/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 0.0399 - mean_absolute_error: 0.0399 - val_loss: 0.0817 - val_mean_absolute_error: 0.0817\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.02301\n",
            "Epoch 233/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0874 - mean_absolute_error: 0.0874 - val_loss: 0.0645 - val_mean_absolute_error: 0.0645\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.02301\n",
            "Epoch 234/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0478 - mean_absolute_error: 0.0478 - val_loss: 0.0428 - val_mean_absolute_error: 0.0428\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.02301\n",
            "Epoch 235/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0487 - mean_absolute_error: 0.0487 - val_loss: 0.0639 - val_mean_absolute_error: 0.0639\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.02301\n",
            "Epoch 236/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0679 - mean_absolute_error: 0.0679 - val_loss: 0.1361 - val_mean_absolute_error: 0.1361\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.02301\n",
            "Epoch 237/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0892 - mean_absolute_error: 0.0892 - val_loss: 0.0999 - val_mean_absolute_error: 0.0999\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.02301\n",
            "Epoch 238/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0757 - mean_absolute_error: 0.0757 - val_loss: 0.0950 - val_mean_absolute_error: 0.0950\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.02301\n",
            "Epoch 239/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0944 - mean_absolute_error: 0.0944 - val_loss: 0.0489 - val_mean_absolute_error: 0.0489\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.02301\n",
            "Epoch 240/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0575 - mean_absolute_error: 0.0575 - val_loss: 0.0710 - val_mean_absolute_error: 0.0710\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.02301\n",
            "Epoch 241/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0616 - mean_absolute_error: 0.0616 - val_loss: 0.0851 - val_mean_absolute_error: 0.0851\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.02301\n",
            "Epoch 242/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0565 - mean_absolute_error: 0.0565 - val_loss: 0.0272 - val_mean_absolute_error: 0.0272\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.02301\n",
            "Epoch 243/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0608 - val_mean_absolute_error: 0.0608\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.02301\n",
            "Epoch 244/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0746 - mean_absolute_error: 0.0746 - val_loss: 0.0646 - val_mean_absolute_error: 0.0646\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.02301\n",
            "Epoch 245/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0882 - mean_absolute_error: 0.0882 - val_loss: 0.0619 - val_mean_absolute_error: 0.0619\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.02301\n",
            "Epoch 246/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 0.0476 - mean_absolute_error: 0.0476 - val_loss: 0.0304 - val_mean_absolute_error: 0.0304\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.02301\n",
            "Epoch 247/500\n",
            "1376/1376 [==============================] - 0s 82us/step - loss: 0.0757 - mean_absolute_error: 0.0757 - val_loss: 0.0291 - val_mean_absolute_error: 0.0291\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.02301\n",
            "Epoch 248/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0339 - mean_absolute_error: 0.0339 - val_loss: 0.0527 - val_mean_absolute_error: 0.0527\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.02301\n",
            "Epoch 249/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0519 - mean_absolute_error: 0.0519 - val_loss: 0.0288 - val_mean_absolute_error: 0.0288\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.02301\n",
            "Epoch 250/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0693 - mean_absolute_error: 0.0693 - val_loss: 0.0622 - val_mean_absolute_error: 0.0622\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.02301\n",
            "Epoch 251/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0625 - mean_absolute_error: 0.0625 - val_loss: 0.1145 - val_mean_absolute_error: 0.1145\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.02301\n",
            "Epoch 252/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0666 - mean_absolute_error: 0.0666 - val_loss: 0.0405 - val_mean_absolute_error: 0.0405\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.02301\n",
            "Epoch 253/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0573 - mean_absolute_error: 0.0573 - val_loss: 0.0457 - val_mean_absolute_error: 0.0457\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.02301\n",
            "Epoch 254/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0484 - mean_absolute_error: 0.0484 - val_loss: 0.0341 - val_mean_absolute_error: 0.0341\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.02301\n",
            "Epoch 255/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0645 - mean_absolute_error: 0.0645 - val_loss: 0.0812 - val_mean_absolute_error: 0.0812\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.02301\n",
            "Epoch 256/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0657 - mean_absolute_error: 0.0657 - val_loss: 0.0916 - val_mean_absolute_error: 0.0916\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.02301\n",
            "Epoch 257/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0576 - mean_absolute_error: 0.0576 - val_loss: 0.0457 - val_mean_absolute_error: 0.0457\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.02301\n",
            "Epoch 258/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0721 - mean_absolute_error: 0.0721 - val_loss: 0.0521 - val_mean_absolute_error: 0.0521\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.02301\n",
            "Epoch 259/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0430 - mean_absolute_error: 0.0430 - val_loss: 0.0268 - val_mean_absolute_error: 0.0268\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.02301\n",
            "Epoch 260/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0492 - mean_absolute_error: 0.0492 - val_loss: 0.0533 - val_mean_absolute_error: 0.0533\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.02301\n",
            "Epoch 261/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0611 - mean_absolute_error: 0.0611 - val_loss: 0.0523 - val_mean_absolute_error: 0.0523\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.02301\n",
            "Epoch 262/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0395 - mean_absolute_error: 0.0395 - val_loss: 0.0362 - val_mean_absolute_error: 0.0362\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.02301\n",
            "Epoch 263/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0594 - mean_absolute_error: 0.0594 - val_loss: 0.0720 - val_mean_absolute_error: 0.0720\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.02301\n",
            "Epoch 264/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0547 - mean_absolute_error: 0.0547 - val_loss: 0.0527 - val_mean_absolute_error: 0.0527\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.02301\n",
            "Epoch 265/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0678 - mean_absolute_error: 0.0678 - val_loss: 0.0843 - val_mean_absolute_error: 0.0843\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.02301\n",
            "Epoch 266/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 0.0450 - mean_absolute_error: 0.0450 - val_loss: 0.0422 - val_mean_absolute_error: 0.0422\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.02301\n",
            "Epoch 267/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0981 - mean_absolute_error: 0.0981 - val_loss: 0.1234 - val_mean_absolute_error: 0.1234\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.02301\n",
            "Epoch 268/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.1032 - mean_absolute_error: 0.1032 - val_loss: 0.0520 - val_mean_absolute_error: 0.0520\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.02301\n",
            "Epoch 269/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0685 - mean_absolute_error: 0.0685 - val_loss: 0.0597 - val_mean_absolute_error: 0.0597\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.02301\n",
            "Epoch 270/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0664 - mean_absolute_error: 0.0664 - val_loss: 0.0778 - val_mean_absolute_error: 0.0778\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.02301\n",
            "Epoch 271/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0843 - mean_absolute_error: 0.0843 - val_loss: 0.1193 - val_mean_absolute_error: 0.1193\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.02301\n",
            "Epoch 272/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0949 - mean_absolute_error: 0.0949 - val_loss: 0.0947 - val_mean_absolute_error: 0.0947\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.02301\n",
            "Epoch 273/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0944 - mean_absolute_error: 0.0944 - val_loss: 0.0627 - val_mean_absolute_error: 0.0627\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.02301\n",
            "Epoch 274/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 0.0772 - mean_absolute_error: 0.0772 - val_loss: 0.0729 - val_mean_absolute_error: 0.0729\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.02301\n",
            "Epoch 275/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0861 - mean_absolute_error: 0.0861 - val_loss: 0.0317 - val_mean_absolute_error: 0.0317\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.02301\n",
            "Epoch 276/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0679 - mean_absolute_error: 0.0679 - val_loss: 0.0436 - val_mean_absolute_error: 0.0436\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.02301\n",
            "Epoch 277/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0561 - mean_absolute_error: 0.0561 - val_loss: 0.0496 - val_mean_absolute_error: 0.0496\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.02301\n",
            "Epoch 278/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0544 - mean_absolute_error: 0.0544 - val_loss: 0.0497 - val_mean_absolute_error: 0.0497\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.02301\n",
            "Epoch 279/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0476 - mean_absolute_error: 0.0476 - val_loss: 0.0834 - val_mean_absolute_error: 0.0834\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.02301\n",
            "Epoch 280/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0576 - mean_absolute_error: 0.0576 - val_loss: 0.0412 - val_mean_absolute_error: 0.0412\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.02301\n",
            "Epoch 281/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0557 - mean_absolute_error: 0.0557 - val_loss: 0.0487 - val_mean_absolute_error: 0.0487\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.02301\n",
            "Epoch 282/500\n",
            "1376/1376 [==============================] - 0s 98us/step - loss: 0.0466 - mean_absolute_error: 0.0466 - val_loss: 0.0545 - val_mean_absolute_error: 0.0545\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.02301\n",
            "Epoch 283/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0592 - mean_absolute_error: 0.0592 - val_loss: 0.0581 - val_mean_absolute_error: 0.0581\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.02301\n",
            "Epoch 284/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0734 - mean_absolute_error: 0.0734 - val_loss: 0.1075 - val_mean_absolute_error: 0.1075\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.02301\n",
            "Epoch 285/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0760 - mean_absolute_error: 0.0760 - val_loss: 0.0432 - val_mean_absolute_error: 0.0432\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.02301\n",
            "Epoch 286/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1253 - mean_absolute_error: 0.1253 - val_loss: 0.0769 - val_mean_absolute_error: 0.0769\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.02301\n",
            "Epoch 287/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0820 - mean_absolute_error: 0.0820 - val_loss: 0.1438 - val_mean_absolute_error: 0.1438\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.02301\n",
            "Epoch 288/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0887 - mean_absolute_error: 0.0887 - val_loss: 0.1369 - val_mean_absolute_error: 0.1369\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.02301\n",
            "Epoch 289/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 0.1390 - mean_absolute_error: 0.1390 - val_loss: 0.0525 - val_mean_absolute_error: 0.0525\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.02301\n",
            "Epoch 290/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0960 - mean_absolute_error: 0.0960 - val_loss: 0.0355 - val_mean_absolute_error: 0.0355\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.02301\n",
            "Epoch 291/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0571 - mean_absolute_error: 0.0571 - val_loss: 0.0493 - val_mean_absolute_error: 0.0493\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.02301\n",
            "Epoch 292/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0645 - mean_absolute_error: 0.0645 - val_loss: 0.0900 - val_mean_absolute_error: 0.0900\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.02301\n",
            "Epoch 293/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0548 - mean_absolute_error: 0.0548 - val_loss: 0.0733 - val_mean_absolute_error: 0.0733\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.02301\n",
            "Epoch 294/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0718 - mean_absolute_error: 0.0718 - val_loss: 0.0750 - val_mean_absolute_error: 0.0750\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.02301\n",
            "Epoch 295/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0883 - mean_absolute_error: 0.0883 - val_loss: 0.1182 - val_mean_absolute_error: 0.1182\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.02301\n",
            "Epoch 296/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0595 - mean_absolute_error: 0.0595 - val_loss: 0.0898 - val_mean_absolute_error: 0.0898\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.02301\n",
            "Epoch 297/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0678 - mean_absolute_error: 0.0678 - val_loss: 0.0553 - val_mean_absolute_error: 0.0553\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.02301\n",
            "Epoch 298/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0670 - mean_absolute_error: 0.0670 - val_loss: 0.0447 - val_mean_absolute_error: 0.0447\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.02301\n",
            "Epoch 299/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0393 - mean_absolute_error: 0.0393 - val_loss: 0.0387 - val_mean_absolute_error: 0.0387\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.02301\n",
            "Epoch 300/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0652 - mean_absolute_error: 0.0652 - val_loss: 0.0871 - val_mean_absolute_error: 0.0871\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.02301\n",
            "Epoch 301/500\n",
            "1376/1376 [==============================] - 0s 82us/step - loss: 0.0950 - mean_absolute_error: 0.0950 - val_loss: 0.1146 - val_mean_absolute_error: 0.1146\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.02301\n",
            "Epoch 302/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0953 - mean_absolute_error: 0.0953 - val_loss: 0.0952 - val_mean_absolute_error: 0.0952\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.02301\n",
            "Epoch 303/500\n",
            "1376/1376 [==============================] - 0s 82us/step - loss: 0.0634 - mean_absolute_error: 0.0634 - val_loss: 0.0292 - val_mean_absolute_error: 0.0292\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.02301\n",
            "Epoch 304/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0750 - mean_absolute_error: 0.0750 - val_loss: 0.1305 - val_mean_absolute_error: 0.1305\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.02301\n",
            "Epoch 305/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.1220 - mean_absolute_error: 0.1220 - val_loss: 0.0938 - val_mean_absolute_error: 0.0938\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.02301\n",
            "Epoch 306/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0674 - mean_absolute_error: 0.0674 - val_loss: 0.0662 - val_mean_absolute_error: 0.0662\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.02301\n",
            "Epoch 307/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0700 - mean_absolute_error: 0.0700 - val_loss: 0.0347 - val_mean_absolute_error: 0.0347\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.02301\n",
            "Epoch 308/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0895 - mean_absolute_error: 0.0895 - val_loss: 0.0902 - val_mean_absolute_error: 0.0902\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.02301\n",
            "Epoch 309/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.1539 - mean_absolute_error: 0.1539 - val_loss: 0.0450 - val_mean_absolute_error: 0.0450\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.02301\n",
            "Epoch 310/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0642 - mean_absolute_error: 0.0642 - val_loss: 0.0449 - val_mean_absolute_error: 0.0449\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.02301\n",
            "Epoch 311/500\n",
            "1376/1376 [==============================] - 0s 82us/step - loss: 0.0454 - mean_absolute_error: 0.0454 - val_loss: 0.0282 - val_mean_absolute_error: 0.0282\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.02301\n",
            "Epoch 312/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0528 - mean_absolute_error: 0.0528 - val_loss: 0.1083 - val_mean_absolute_error: 0.1083\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.02301\n",
            "Epoch 313/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0926 - mean_absolute_error: 0.0926 - val_loss: 0.0531 - val_mean_absolute_error: 0.0531\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.02301\n",
            "Epoch 314/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0446 - mean_absolute_error: 0.0446 - val_loss: 0.0397 - val_mean_absolute_error: 0.0397\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.02301\n",
            "Epoch 315/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0704 - mean_absolute_error: 0.0704 - val_loss: 0.0890 - val_mean_absolute_error: 0.0890\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.02301\n",
            "Epoch 316/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0882 - mean_absolute_error: 0.0882 - val_loss: 0.0961 - val_mean_absolute_error: 0.0961\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.02301\n",
            "Epoch 317/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0926 - mean_absolute_error: 0.0926 - val_loss: 0.0621 - val_mean_absolute_error: 0.0621\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.02301\n",
            "Epoch 318/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0529 - mean_absolute_error: 0.0529 - val_loss: 0.0567 - val_mean_absolute_error: 0.0567\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.02301\n",
            "Epoch 319/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0467 - mean_absolute_error: 0.0467 - val_loss: 0.0283 - val_mean_absolute_error: 0.0283\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.02301\n",
            "Epoch 320/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0344 - mean_absolute_error: 0.0344 - val_loss: 0.0292 - val_mean_absolute_error: 0.0292\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.02301\n",
            "Epoch 321/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 0.0518 - mean_absolute_error: 0.0518 - val_loss: 0.0277 - val_mean_absolute_error: 0.0277\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.02301\n",
            "Epoch 322/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0791 - val_mean_absolute_error: 0.0791\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.02301\n",
            "Epoch 323/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0814 - mean_absolute_error: 0.0814 - val_loss: 0.0405 - val_mean_absolute_error: 0.0405\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.02301\n",
            "Epoch 324/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0340 - val_mean_absolute_error: 0.0340\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.02301\n",
            "Epoch 325/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0546 - mean_absolute_error: 0.0546 - val_loss: 0.0634 - val_mean_absolute_error: 0.0634\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.02301\n",
            "Epoch 326/500\n",
            "1376/1376 [==============================] - 0s 84us/step - loss: 0.0388 - mean_absolute_error: 0.0388 - val_loss: 0.0327 - val_mean_absolute_error: 0.0327\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.02301\n",
            "Epoch 327/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0383 - mean_absolute_error: 0.0383 - val_loss: 0.1161 - val_mean_absolute_error: 0.1161\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.02301\n",
            "Epoch 328/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1177 - mean_absolute_error: 0.1177 - val_loss: 0.0518 - val_mean_absolute_error: 0.0518\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.02301\n",
            "Epoch 329/500\n",
            "1376/1376 [==============================] - 0s 86us/step - loss: 0.0598 - mean_absolute_error: 0.0598 - val_loss: 0.0777 - val_mean_absolute_error: 0.0777\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.02301\n",
            "Epoch 330/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0591 - mean_absolute_error: 0.0591 - val_loss: 0.0346 - val_mean_absolute_error: 0.0346\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.02301\n",
            "Epoch 331/500\n",
            "1376/1376 [==============================] - 0s 82us/step - loss: 0.0558 - mean_absolute_error: 0.0558 - val_loss: 0.0693 - val_mean_absolute_error: 0.0693\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.02301\n",
            "Epoch 332/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0616 - mean_absolute_error: 0.0616 - val_loss: 0.0509 - val_mean_absolute_error: 0.0509\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.02301\n",
            "Epoch 333/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0570 - mean_absolute_error: 0.0570 - val_loss: 0.1058 - val_mean_absolute_error: 0.1058\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.02301\n",
            "Epoch 334/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.1032 - mean_absolute_error: 0.1032 - val_loss: 0.0778 - val_mean_absolute_error: 0.0778\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.02301\n",
            "Epoch 335/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0731 - mean_absolute_error: 0.0731 - val_loss: 0.0608 - val_mean_absolute_error: 0.0608\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.02301\n",
            "Epoch 336/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0598 - mean_absolute_error: 0.0598 - val_loss: 0.0497 - val_mean_absolute_error: 0.0497\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.02301\n",
            "Epoch 337/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0452 - mean_absolute_error: 0.0452 - val_loss: 0.0313 - val_mean_absolute_error: 0.0313\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.02301\n",
            "Epoch 338/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0416 - mean_absolute_error: 0.0416 - val_loss: 0.0502 - val_mean_absolute_error: 0.0502\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.02301\n",
            "Epoch 339/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0417 - mean_absolute_error: 0.0417 - val_loss: 0.0426 - val_mean_absolute_error: 0.0426\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.02301\n",
            "Epoch 340/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0547 - mean_absolute_error: 0.0547 - val_loss: 0.0589 - val_mean_absolute_error: 0.0589\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.02301\n",
            "Epoch 341/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0604 - mean_absolute_error: 0.0604 - val_loss: 0.0667 - val_mean_absolute_error: 0.0667\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.02301\n",
            "Epoch 342/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0742 - mean_absolute_error: 0.0742 - val_loss: 0.0358 - val_mean_absolute_error: 0.0358\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.02301\n",
            "Epoch 343/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0532 - mean_absolute_error: 0.0532 - val_loss: 0.0545 - val_mean_absolute_error: 0.0545\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.02301\n",
            "Epoch 344/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0724 - mean_absolute_error: 0.0724 - val_loss: 0.0952 - val_mean_absolute_error: 0.0952\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.02301\n",
            "Epoch 345/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0764 - mean_absolute_error: 0.0764 - val_loss: 0.0425 - val_mean_absolute_error: 0.0425\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.02301\n",
            "Epoch 346/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0840 - mean_absolute_error: 0.0840 - val_loss: 0.0361 - val_mean_absolute_error: 0.0361\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.02301\n",
            "Epoch 347/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0643 - mean_absolute_error: 0.0643 - val_loss: 0.0391 - val_mean_absolute_error: 0.0391\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.02301\n",
            "Epoch 348/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0974 - mean_absolute_error: 0.0974 - val_loss: 0.0867 - val_mean_absolute_error: 0.0867\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.02301\n",
            "Epoch 349/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.1031 - mean_absolute_error: 0.1031 - val_loss: 0.0976 - val_mean_absolute_error: 0.0976\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.02301\n",
            "Epoch 350/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0726 - mean_absolute_error: 0.0726 - val_loss: 0.0746 - val_mean_absolute_error: 0.0746\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.02301\n",
            "Epoch 351/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0834 - mean_absolute_error: 0.0834 - val_loss: 0.0609 - val_mean_absolute_error: 0.0609\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.02301\n",
            "Epoch 352/500\n",
            "1376/1376 [==============================] - 0s 84us/step - loss: 0.0416 - mean_absolute_error: 0.0416 - val_loss: 0.0595 - val_mean_absolute_error: 0.0595\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.02301\n",
            "Epoch 353/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0531 - mean_absolute_error: 0.0531 - val_loss: 0.0793 - val_mean_absolute_error: 0.0793\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.02301\n",
            "Epoch 354/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0848 - mean_absolute_error: 0.0848 - val_loss: 0.1128 - val_mean_absolute_error: 0.1128\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.02301\n",
            "Epoch 355/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 0.1016 - mean_absolute_error: 0.1016 - val_loss: 0.1099 - val_mean_absolute_error: 0.1099\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.02301\n",
            "Epoch 356/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0963 - mean_absolute_error: 0.0963 - val_loss: 0.0696 - val_mean_absolute_error: 0.0696\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.02301\n",
            "Epoch 357/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0674 - mean_absolute_error: 0.0674 - val_loss: 0.0961 - val_mean_absolute_error: 0.0961\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.02301\n",
            "Epoch 358/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 0.0571 - mean_absolute_error: 0.0571 - val_loss: 0.0352 - val_mean_absolute_error: 0.0352\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.02301\n",
            "Epoch 359/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0645 - mean_absolute_error: 0.0645 - val_loss: 0.0873 - val_mean_absolute_error: 0.0873\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.02301\n",
            "Epoch 360/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0639 - mean_absolute_error: 0.0639 - val_loss: 0.0381 - val_mean_absolute_error: 0.0381\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.02301\n",
            "Epoch 361/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0484 - mean_absolute_error: 0.0484 - val_loss: 0.0451 - val_mean_absolute_error: 0.0451\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.02301\n",
            "Epoch 362/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0481 - mean_absolute_error: 0.0481 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.02301\n",
            "Epoch 363/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.1317 - mean_absolute_error: 0.1317 - val_loss: 0.1100 - val_mean_absolute_error: 0.1100\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.02301\n",
            "Epoch 364/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.1131 - mean_absolute_error: 0.1131 - val_loss: 0.0423 - val_mean_absolute_error: 0.0423\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.02301\n",
            "Epoch 365/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0372 - mean_absolute_error: 0.0372 - val_loss: 0.0312 - val_mean_absolute_error: 0.0312\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.02301\n",
            "Epoch 366/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0434 - mean_absolute_error: 0.0434 - val_loss: 0.0355 - val_mean_absolute_error: 0.0355\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.02301\n",
            "Epoch 367/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0492 - mean_absolute_error: 0.0492 - val_loss: 0.0393 - val_mean_absolute_error: 0.0393\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.02301\n",
            "Epoch 368/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0754 - mean_absolute_error: 0.0754 - val_loss: 0.1075 - val_mean_absolute_error: 0.1075\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.02301\n",
            "Epoch 369/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0670 - mean_absolute_error: 0.0670 - val_loss: 0.0481 - val_mean_absolute_error: 0.0481\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.02301\n",
            "Epoch 370/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0415 - mean_absolute_error: 0.0415 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.02301\n",
            "Epoch 371/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 0.0424 - mean_absolute_error: 0.0424 - val_loss: 0.0608 - val_mean_absolute_error: 0.0608\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.02301\n",
            "Epoch 372/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0918 - mean_absolute_error: 0.0918 - val_loss: 0.0414 - val_mean_absolute_error: 0.0414\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.02301\n",
            "Epoch 373/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0855 - mean_absolute_error: 0.0855 - val_loss: 0.0853 - val_mean_absolute_error: 0.0853\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.02301\n",
            "Epoch 374/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0788 - mean_absolute_error: 0.0788 - val_loss: 0.0769 - val_mean_absolute_error: 0.0769\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.02301\n",
            "Epoch 375/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 0.0644 - mean_absolute_error: 0.0644 - val_loss: 0.0320 - val_mean_absolute_error: 0.0320\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.02301\n",
            "Epoch 376/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0596 - mean_absolute_error: 0.0596 - val_loss: 0.0781 - val_mean_absolute_error: 0.0781\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.02301\n",
            "Epoch 377/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0834 - mean_absolute_error: 0.0834 - val_loss: 0.0582 - val_mean_absolute_error: 0.0582\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.02301\n",
            "Epoch 378/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0608 - mean_absolute_error: 0.0608 - val_loss: 0.0355 - val_mean_absolute_error: 0.0355\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.02301\n",
            "Epoch 379/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0519 - mean_absolute_error: 0.0519 - val_loss: 0.0626 - val_mean_absolute_error: 0.0626\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.02301\n",
            "Epoch 380/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0675 - mean_absolute_error: 0.0675 - val_loss: 0.0848 - val_mean_absolute_error: 0.0848\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.02301\n",
            "Epoch 381/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0558 - mean_absolute_error: 0.0558 - val_loss: 0.0600 - val_mean_absolute_error: 0.0600\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.02301\n",
            "Epoch 382/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0441 - mean_absolute_error: 0.0441 - val_loss: 0.0613 - val_mean_absolute_error: 0.0613\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.02301\n",
            "Epoch 383/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0720 - mean_absolute_error: 0.0720 - val_loss: 0.0708 - val_mean_absolute_error: 0.0708\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.02301\n",
            "Epoch 384/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0435 - mean_absolute_error: 0.0435 - val_loss: 0.0339 - val_mean_absolute_error: 0.0339\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.02301\n",
            "Epoch 385/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0404 - mean_absolute_error: 0.0404 - val_loss: 0.0625 - val_mean_absolute_error: 0.0625\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.02301\n",
            "Epoch 386/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0435 - mean_absolute_error: 0.0435 - val_loss: 0.0608 - val_mean_absolute_error: 0.0608\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.02301\n",
            "Epoch 387/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0707 - mean_absolute_error: 0.0707 - val_loss: 0.1224 - val_mean_absolute_error: 0.1224\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.02301\n",
            "Epoch 388/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.1023 - mean_absolute_error: 0.1023 - val_loss: 0.0601 - val_mean_absolute_error: 0.0601\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.02301\n",
            "Epoch 389/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0446 - mean_absolute_error: 0.0446 - val_loss: 0.0636 - val_mean_absolute_error: 0.0636\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.02301\n",
            "Epoch 390/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 0.0614 - mean_absolute_error: 0.0614 - val_loss: 0.0758 - val_mean_absolute_error: 0.0758\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.02301\n",
            "Epoch 391/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0604 - mean_absolute_error: 0.0604 - val_loss: 0.0566 - val_mean_absolute_error: 0.0566\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.02301\n",
            "Epoch 392/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0830 - mean_absolute_error: 0.0830 - val_loss: 0.1213 - val_mean_absolute_error: 0.1213\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.02301\n",
            "Epoch 393/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0794 - mean_absolute_error: 0.0794 - val_loss: 0.0915 - val_mean_absolute_error: 0.0915\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.02301\n",
            "Epoch 394/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0886 - mean_absolute_error: 0.0886 - val_loss: 0.0941 - val_mean_absolute_error: 0.0941\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.02301\n",
            "Epoch 395/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0672 - mean_absolute_error: 0.0672 - val_loss: 0.0806 - val_mean_absolute_error: 0.0806\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.02301\n",
            "Epoch 396/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0528 - mean_absolute_error: 0.0528 - val_loss: 0.0633 - val_mean_absolute_error: 0.0633\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.02301\n",
            "Epoch 397/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 0.0495 - mean_absolute_error: 0.0495 - val_loss: 0.0438 - val_mean_absolute_error: 0.0438\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.02301\n",
            "Epoch 398/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0465 - mean_absolute_error: 0.0465 - val_loss: 0.0422 - val_mean_absolute_error: 0.0422\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.02301\n",
            "Epoch 399/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0579 - mean_absolute_error: 0.0579 - val_loss: 0.0329 - val_mean_absolute_error: 0.0329\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.02301\n",
            "Epoch 400/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0705 - mean_absolute_error: 0.0705 - val_loss: 0.0699 - val_mean_absolute_error: 0.0699\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.02301\n",
            "Epoch 401/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0795 - mean_absolute_error: 0.0795 - val_loss: 0.0817 - val_mean_absolute_error: 0.0817\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.02301\n",
            "Epoch 402/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0631 - mean_absolute_error: 0.0631 - val_loss: 0.0464 - val_mean_absolute_error: 0.0464\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.02301\n",
            "Epoch 403/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0479 - mean_absolute_error: 0.0479 - val_loss: 0.0622 - val_mean_absolute_error: 0.0622\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.02301\n",
            "Epoch 404/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0530 - mean_absolute_error: 0.0530 - val_loss: 0.0604 - val_mean_absolute_error: 0.0604\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.02301\n",
            "Epoch 405/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0386 - mean_absolute_error: 0.0386 - val_loss: 0.0498 - val_mean_absolute_error: 0.0498\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.02301\n",
            "Epoch 406/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0532 - mean_absolute_error: 0.0532 - val_loss: 0.0540 - val_mean_absolute_error: 0.0540\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.02301\n",
            "Epoch 407/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0647 - mean_absolute_error: 0.0647 - val_loss: 0.1164 - val_mean_absolute_error: 0.1164\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.02301\n",
            "Epoch 408/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0609 - mean_absolute_error: 0.0609 - val_loss: 0.0492 - val_mean_absolute_error: 0.0492\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.02301\n",
            "Epoch 409/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0567 - mean_absolute_error: 0.0567 - val_loss: 0.1163 - val_mean_absolute_error: 0.1163\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.02301\n",
            "Epoch 410/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.1014 - mean_absolute_error: 0.1014 - val_loss: 0.0641 - val_mean_absolute_error: 0.0641\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.02301\n",
            "Epoch 411/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 0.0640 - mean_absolute_error: 0.0640 - val_loss: 0.0441 - val_mean_absolute_error: 0.0441\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.02301\n",
            "Epoch 412/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0704 - mean_absolute_error: 0.0704 - val_loss: 0.0662 - val_mean_absolute_error: 0.0662\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.02301\n",
            "Epoch 413/500\n",
            "1376/1376 [==============================] - 0s 67us/step - loss: 0.0635 - mean_absolute_error: 0.0635 - val_loss: 0.0527 - val_mean_absolute_error: 0.0527\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.02301\n",
            "Epoch 414/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0725 - mean_absolute_error: 0.0725 - val_loss: 0.0741 - val_mean_absolute_error: 0.0741\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.02301\n",
            "Epoch 415/500\n",
            "1376/1376 [==============================] - 0s 85us/step - loss: 0.0559 - mean_absolute_error: 0.0559 - val_loss: 0.0307 - val_mean_absolute_error: 0.0307\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.02301\n",
            "Epoch 416/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0494 - mean_absolute_error: 0.0494 - val_loss: 0.0583 - val_mean_absolute_error: 0.0583\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.02301\n",
            "Epoch 417/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0604 - mean_absolute_error: 0.0604 - val_loss: 0.0273 - val_mean_absolute_error: 0.0273\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.02301\n",
            "Epoch 418/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0731 - mean_absolute_error: 0.0731 - val_loss: 0.0443 - val_mean_absolute_error: 0.0443\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.02301\n",
            "Epoch 419/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0467 - mean_absolute_error: 0.0467 - val_loss: 0.0501 - val_mean_absolute_error: 0.0501\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.02301\n",
            "Epoch 420/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 0.0546 - mean_absolute_error: 0.0546 - val_loss: 0.0413 - val_mean_absolute_error: 0.0413\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.02301\n",
            "Epoch 421/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0513 - mean_absolute_error: 0.0513 - val_loss: 0.0881 - val_mean_absolute_error: 0.0881\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.02301\n",
            "Epoch 422/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0755 - mean_absolute_error: 0.0755 - val_loss: 0.0487 - val_mean_absolute_error: 0.0487\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.02301\n",
            "Epoch 423/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0489 - mean_absolute_error: 0.0489 - val_loss: 0.0292 - val_mean_absolute_error: 0.0292\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.02301\n",
            "Epoch 424/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 0.0330 - mean_absolute_error: 0.0330 - val_loss: 0.0156 - val_mean_absolute_error: 0.0156\n",
            "\n",
            "Epoch 00424: val_loss improved from 0.02301 to 0.01564, saving model to Weights-424--0.01564.hdf5\n",
            "Epoch 425/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0567 - mean_absolute_error: 0.0567 - val_loss: 0.0628 - val_mean_absolute_error: 0.0628\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.01564\n",
            "Epoch 426/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0640 - mean_absolute_error: 0.0640 - val_loss: 0.0844 - val_mean_absolute_error: 0.0844\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.01564\n",
            "Epoch 427/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0516 - mean_absolute_error: 0.0516 - val_loss: 0.0307 - val_mean_absolute_error: 0.0307\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.01564\n",
            "Epoch 428/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0838 - mean_absolute_error: 0.0838 - val_loss: 0.0603 - val_mean_absolute_error: 0.0603\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.01564\n",
            "Epoch 429/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0893 - mean_absolute_error: 0.0893 - val_loss: 0.0827 - val_mean_absolute_error: 0.0827\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.01564\n",
            "Epoch 430/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0665 - mean_absolute_error: 0.0665 - val_loss: 0.0497 - val_mean_absolute_error: 0.0497\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.01564\n",
            "Epoch 431/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0460 - mean_absolute_error: 0.0460 - val_loss: 0.0431 - val_mean_absolute_error: 0.0431\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.01564\n",
            "Epoch 432/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0432 - mean_absolute_error: 0.0432 - val_loss: 0.0360 - val_mean_absolute_error: 0.0360\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.01564\n",
            "Epoch 433/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0909 - mean_absolute_error: 0.0909 - val_loss: 0.0465 - val_mean_absolute_error: 0.0465\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.01564\n",
            "Epoch 434/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0688 - mean_absolute_error: 0.0688 - val_loss: 0.0511 - val_mean_absolute_error: 0.0511\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.01564\n",
            "Epoch 435/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0497 - mean_absolute_error: 0.0497 - val_loss: 0.0493 - val_mean_absolute_error: 0.0493\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.01564\n",
            "Epoch 436/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0468 - mean_absolute_error: 0.0468 - val_loss: 0.0449 - val_mean_absolute_error: 0.0449\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.01564\n",
            "Epoch 437/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0522 - mean_absolute_error: 0.0522 - val_loss: 0.0286 - val_mean_absolute_error: 0.0286\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.01564\n",
            "Epoch 438/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0444 - mean_absolute_error: 0.0444 - val_loss: 0.0793 - val_mean_absolute_error: 0.0793\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.01564\n",
            "Epoch 439/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0543 - mean_absolute_error: 0.0543 - val_loss: 0.0993 - val_mean_absolute_error: 0.0993\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.01564\n",
            "Epoch 440/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0542 - mean_absolute_error: 0.0542 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.01564\n",
            "Epoch 441/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0851 - mean_absolute_error: 0.0851 - val_loss: 0.1192 - val_mean_absolute_error: 0.1192\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.01564\n",
            "Epoch 442/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0885 - mean_absolute_error: 0.0885 - val_loss: 0.0715 - val_mean_absolute_error: 0.0715\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.01564\n",
            "Epoch 443/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0740 - mean_absolute_error: 0.0740 - val_loss: 0.0423 - val_mean_absolute_error: 0.0423\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.01564\n",
            "Epoch 444/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0471 - mean_absolute_error: 0.0471 - val_loss: 0.0354 - val_mean_absolute_error: 0.0354\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.01564\n",
            "Epoch 445/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0481 - mean_absolute_error: 0.0481 - val_loss: 0.0349 - val_mean_absolute_error: 0.0349\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.01564\n",
            "Epoch 446/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0403 - mean_absolute_error: 0.0403 - val_loss: 0.0353 - val_mean_absolute_error: 0.0353\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.01564\n",
            "Epoch 447/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0298 - val_mean_absolute_error: 0.0298\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.01564\n",
            "Epoch 448/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0739 - mean_absolute_error: 0.0739 - val_loss: 0.1058 - val_mean_absolute_error: 0.1058\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.01564\n",
            "Epoch 449/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0743 - mean_absolute_error: 0.0743 - val_loss: 0.0311 - val_mean_absolute_error: 0.0311\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.01564\n",
            "Epoch 450/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0610 - mean_absolute_error: 0.0610 - val_loss: 0.0553 - val_mean_absolute_error: 0.0553\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.01564\n",
            "Epoch 451/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0576 - mean_absolute_error: 0.0576 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.01564\n",
            "Epoch 452/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0439 - mean_absolute_error: 0.0439 - val_loss: 0.0444 - val_mean_absolute_error: 0.0444\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.01564\n",
            "Epoch 453/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 0.0599 - mean_absolute_error: 0.0599 - val_loss: 0.0649 - val_mean_absolute_error: 0.0649\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.01564\n",
            "Epoch 454/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0802 - mean_absolute_error: 0.0802 - val_loss: 0.0409 - val_mean_absolute_error: 0.0409\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.01564\n",
            "Epoch 455/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0432 - mean_absolute_error: 0.0432 - val_loss: 0.0258 - val_mean_absolute_error: 0.0258\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.01564\n",
            "Epoch 456/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 0.0375 - mean_absolute_error: 0.0375 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.01564\n",
            "Epoch 457/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0791 - mean_absolute_error: 0.0791 - val_loss: 0.1404 - val_mean_absolute_error: 0.1404\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.01564\n",
            "Epoch 458/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.1287 - mean_absolute_error: 0.1287 - val_loss: 0.1265 - val_mean_absolute_error: 0.1265\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.01564\n",
            "Epoch 459/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0868 - mean_absolute_error: 0.0868 - val_loss: 0.0613 - val_mean_absolute_error: 0.0613\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.01564\n",
            "Epoch 460/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0596 - mean_absolute_error: 0.0596 - val_loss: 0.0489 - val_mean_absolute_error: 0.0489\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.01564\n",
            "Epoch 461/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0864 - mean_absolute_error: 0.0864 - val_loss: 0.1499 - val_mean_absolute_error: 0.1499\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.01564\n",
            "Epoch 462/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 0.1106 - mean_absolute_error: 0.1106 - val_loss: 0.0578 - val_mean_absolute_error: 0.0578\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.01564\n",
            "Epoch 463/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0497 - mean_absolute_error: 0.0497 - val_loss: 0.1170 - val_mean_absolute_error: 0.1170\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.01564\n",
            "Epoch 464/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0907 - mean_absolute_error: 0.0907 - val_loss: 0.0401 - val_mean_absolute_error: 0.0401\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.01564\n",
            "Epoch 465/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 0.0512 - mean_absolute_error: 0.0512 - val_loss: 0.0724 - val_mean_absolute_error: 0.0724\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.01564\n",
            "Epoch 466/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0673 - mean_absolute_error: 0.0673 - val_loss: 0.0508 - val_mean_absolute_error: 0.0508\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.01564\n",
            "Epoch 467/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 0.0405 - mean_absolute_error: 0.0405 - val_loss: 0.0331 - val_mean_absolute_error: 0.0331\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.01564\n",
            "Epoch 468/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 0.0446 - mean_absolute_error: 0.0446 - val_loss: 0.0392 - val_mean_absolute_error: 0.0392\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.01564\n",
            "Epoch 469/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0414 - mean_absolute_error: 0.0414 - val_loss: 0.0350 - val_mean_absolute_error: 0.0350\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.01564\n",
            "Epoch 470/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0445 - mean_absolute_error: 0.0445 - val_loss: 0.0720 - val_mean_absolute_error: 0.0720\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.01564\n",
            "Epoch 471/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.1024 - mean_absolute_error: 0.1024 - val_loss: 0.1051 - val_mean_absolute_error: 0.1051\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.01564\n",
            "Epoch 472/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.1115 - mean_absolute_error: 0.1115 - val_loss: 0.1541 - val_mean_absolute_error: 0.1541\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.01564\n",
            "Epoch 473/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.1033 - mean_absolute_error: 0.1033 - val_loss: 0.0424 - val_mean_absolute_error: 0.0424\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.01564\n",
            "Epoch 474/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0832 - mean_absolute_error: 0.0832 - val_loss: 0.1199 - val_mean_absolute_error: 0.1199\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.01564\n",
            "Epoch 475/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 0.0701 - mean_absolute_error: 0.0701 - val_loss: 0.0873 - val_mean_absolute_error: 0.0873\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.01564\n",
            "Epoch 476/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0805 - mean_absolute_error: 0.0805 - val_loss: 0.1034 - val_mean_absolute_error: 0.1034\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.01564\n",
            "Epoch 477/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0609 - mean_absolute_error: 0.0609 - val_loss: 0.0544 - val_mean_absolute_error: 0.0544\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.01564\n",
            "Epoch 478/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 0.0656 - mean_absolute_error: 0.0656 - val_loss: 0.1092 - val_mean_absolute_error: 0.1092\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.01564\n",
            "Epoch 479/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 0.0939 - mean_absolute_error: 0.0939 - val_loss: 0.0832 - val_mean_absolute_error: 0.0832\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.01564\n",
            "Epoch 480/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0893 - mean_absolute_error: 0.0893 - val_loss: 0.0562 - val_mean_absolute_error: 0.0562\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.01564\n",
            "Epoch 481/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0576 - mean_absolute_error: 0.0576 - val_loss: 0.0874 - val_mean_absolute_error: 0.0874\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.01564\n",
            "Epoch 482/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0662 - mean_absolute_error: 0.0662 - val_loss: 0.0373 - val_mean_absolute_error: 0.0373\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.01564\n",
            "Epoch 483/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0700 - mean_absolute_error: 0.0700 - val_loss: 0.1737 - val_mean_absolute_error: 0.1737\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.01564\n",
            "Epoch 484/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.1509 - mean_absolute_error: 0.1509 - val_loss: 0.0560 - val_mean_absolute_error: 0.0560\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.01564\n",
            "Epoch 485/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 0.0716 - mean_absolute_error: 0.0716 - val_loss: 0.0306 - val_mean_absolute_error: 0.0306\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.01564\n",
            "Epoch 486/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 0.0684 - mean_absolute_error: 0.0684 - val_loss: 0.0632 - val_mean_absolute_error: 0.0632\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.01564\n",
            "Epoch 487/500\n",
            "1376/1376 [==============================] - 0s 82us/step - loss: 0.0644 - mean_absolute_error: 0.0644 - val_loss: 0.1249 - val_mean_absolute_error: 0.1249\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.01564\n",
            "Epoch 488/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0856 - mean_absolute_error: 0.0856 - val_loss: 0.0387 - val_mean_absolute_error: 0.0387\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.01564\n",
            "Epoch 489/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 0.0508 - mean_absolute_error: 0.0508 - val_loss: 0.0231 - val_mean_absolute_error: 0.0231\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.01564\n",
            "Epoch 490/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 0.0467 - mean_absolute_error: 0.0467 - val_loss: 0.0345 - val_mean_absolute_error: 0.0345\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.01564\n",
            "Epoch 491/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0416 - mean_absolute_error: 0.0416 - val_loss: 0.0436 - val_mean_absolute_error: 0.0436\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.01564\n",
            "Epoch 492/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 0.0477 - mean_absolute_error: 0.0477 - val_loss: 0.0989 - val_mean_absolute_error: 0.0989\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.01564\n",
            "Epoch 493/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 0.0574 - mean_absolute_error: 0.0574 - val_loss: 0.0447 - val_mean_absolute_error: 0.0447\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.01564\n",
            "Epoch 494/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 0.0586 - mean_absolute_error: 0.0586 - val_loss: 0.0739 - val_mean_absolute_error: 0.0739\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.01564\n",
            "Epoch 495/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0635 - mean_absolute_error: 0.0635 - val_loss: 0.0567 - val_mean_absolute_error: 0.0567\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.01564\n",
            "Epoch 496/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0413 - mean_absolute_error: 0.0413 - val_loss: 0.0294 - val_mean_absolute_error: 0.0294\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.01564\n",
            "Epoch 497/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 0.0438 - mean_absolute_error: 0.0438 - val_loss: 0.0634 - val_mean_absolute_error: 0.0634\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.01564\n",
            "Epoch 498/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 0.0696 - mean_absolute_error: 0.0696 - val_loss: 0.0369 - val_mean_absolute_error: 0.0369\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.01564\n",
            "Epoch 499/500\n",
            "1376/1376 [==============================] - 0s 84us/step - loss: 0.0751 - mean_absolute_error: 0.0751 - val_loss: 0.0653 - val_mean_absolute_error: 0.0653\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.01564\n",
            "Epoch 500/500\n",
            "1376/1376 [==============================] - 0s 67us/step - loss: 0.0474 - mean_absolute_error: 0.0474 - val_loss: 0.0412 - val_mean_absolute_error: 0.0412\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.01564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f1cf16f9470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbdDCieXidvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}